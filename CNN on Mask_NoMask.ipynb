{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60cac4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f81e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_size = 200  #  you can keep any image size..but image size plays big role in model creation\n",
    "                  # if image size is less..computation needed is less..but at the same time model performance may be bad\n",
    "path1 = r\"D:\\Mask_NoMask\"\n",
    "cate  = ['Mask' ,'No_Mask']\n",
    "    \n",
    "input_image = []    \n",
    "for i in cate:\n",
    "    folders = os.path.join(path1 ,i)\n",
    "    label   = cate.index(i)   # we need to tell software which image is of cat and which is of dog\n",
    "    for image in os.listdir(folders):\n",
    "        image_path  = os.path.join(folders , image)\n",
    "        image_array =  cv2.imread(image_path) #  using the cv2 i am reading the image and storing in variable image_array\n",
    "        image_array = cv2.resize(image_array , (200  , image_size) ) # resizing each image to 200 * 200\n",
    "        input_image.append([image_array , label])\n",
    "                                                      ## lebelis target variable.. label as in category folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef3d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[245, 251, 250],\n",
       "         [245, 251, 250],\n",
       "         [245, 251, 250],\n",
       "         ...,\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253]],\n",
       " \n",
       "        [[244, 251, 249],\n",
       "         [244, 251, 249],\n",
       "         [244, 251, 249],\n",
       "         ...,\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253]],\n",
       " \n",
       "        [[244, 250, 249],\n",
       "         [244, 250, 249],\n",
       "         [244, 250, 249],\n",
       "         ...,\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253],\n",
       "         [254, 255, 253]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 50,  30,  29],\n",
       "         [ 50,  30,  29],\n",
       "         [ 49,  30,  29],\n",
       "         ...,\n",
       "         [ 90,  56,  49],\n",
       "         [ 91,  57,  51],\n",
       "         [ 86,  53,  46]],\n",
       " \n",
       "        [[ 51,  31,  30],\n",
       "         [ 50,  31,  30],\n",
       "         [ 50,  30,  30],\n",
       "         ...,\n",
       "         [ 91,  57,  50],\n",
       "         [ 92,  58,  52],\n",
       "         [ 87,  53,  46]],\n",
       " \n",
       "        [[ 51,  31,  30],\n",
       "         [ 50,  31,  30],\n",
       "         [ 50,  30,  29],\n",
       "         ...,\n",
       "         [ 88,  54,  47],\n",
       "         [ 91,  57,  50],\n",
       "         [ 90,  56,  49]]], dtype=uint8),\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c608e975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8838"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec81e16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 200, 200, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(input_image)\n",
    "\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "for X_values , label in input_image:\n",
    "    X.append(X_values)\n",
    "    Y.append(label)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X_train = X[:7000]    ## 90% in test and 10% in test\n",
    "X_test = X[7000:]\n",
    "\n",
    "Y_train = Y[:7000]\n",
    "Y_test = Y[7000:]\n",
    "\n",
    "X_train = X_train/255  ## 255 is highest pixcel values so we want to do all image in a same pixcel so we have to convert that data into same pixcel\n",
    "X_test = X_test/255    ## we can only do ony on test coz its do on only independent value so here x(array/matrix) is independent value\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce213687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D , MaxPool2D , Flatten , Dense , Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16 , kernel_size =(3,3) , activation ='relu'    ))\n",
    "model.add(MaxPool2D(pool_size =(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128 , activation = 'relu' , input_shape = X.shape[1:]))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2 , activation= 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',   loss = 'sparse_categorical_crossentropy'  , metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d438ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "186/186 [==============================] - 82s 437ms/step - loss: 1.1956 - accuracy: 0.7546 - val_loss: 0.3340 - val_accuracy: 0.8638\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 72s 389ms/step - loss: 0.3216 - accuracy: 0.8729 - val_loss: 0.2437 - val_accuracy: 0.9010\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 76s 407ms/step - loss: 0.2516 - accuracy: 0.9002 - val_loss: 0.2166 - val_accuracy: 0.9162\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 77s 416ms/step - loss: 0.1916 - accuracy: 0.9269 - val_loss: 0.2071 - val_accuracy: 0.9248\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 76s 406ms/step - loss: 0.1580 - accuracy: 0.9370 - val_loss: 0.2022 - val_accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 75s 403ms/step - loss: 0.1200 - accuracy: 0.9551 - val_loss: 0.2416 - val_accuracy: 0.9162\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 77s 412ms/step - loss: 0.1029 - accuracy: 0.9603 - val_loss: 0.2559 - val_accuracy: 0.9238\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 82s 441ms/step - loss: 0.0840 - accuracy: 0.9677 - val_loss: 0.2358 - val_accuracy: 0.9276\n",
      "Epoch 9/10\n",
      "186/186 [==============================] - 79s 424ms/step - loss: 0.0774 - accuracy: 0.9689 - val_loss: 0.2756 - val_accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "186/186 [==============================] - 80s 432ms/step - loss: 0.0738 - accuracy: 0.9708 - val_loss: 0.3312 - val_accuracy: 0.9133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f510e1c730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , Y_train, epochs = 10 , validation_split = .15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a236bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_nomask.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9473b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9996793e-01, 3.2027379e-05],\n",
       "       [9.9265844e-01, 7.3415721e-03],\n",
       "       [9.9867105e-01, 1.3290207e-03],\n",
       "       ...,\n",
       "       [2.5744203e-01, 7.4255800e-01],\n",
       "       [1.2726763e-02, 9.8727322e-01],\n",
       "       [9.9732506e-01, 2.6749198e-03]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred =model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3edc204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[957, 138],\n",
       "       [ 29, 714]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tab = confusion_matrix(np.argmax(pred ,1) ,Y_test)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f65cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9091403699673558"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(pred ,1) , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb1dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cfbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd796b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c96914d",
   "metadata": {},
   "source": [
    "## How to save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf2b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save ('mask_nomask.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df89ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model            ## required import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bcc2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abcd = load_model('Mask_NoMask.h5')\n",
    "#abcd.predict(input data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59afa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd232029",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049fd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
